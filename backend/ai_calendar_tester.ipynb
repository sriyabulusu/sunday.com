{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lmformatenforcer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mai_calendar_processor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AICalendarProcessor\n\u001b[1;32m      3\u001b[0m processor \u001b[38;5;241m=\u001b[39m AICalendarProcessor()\n",
      "File \u001b[0;32m~/Documents/Programming/sunday.com/backend/ai_calendar_processor.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama, LogitsProcessorList\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlmformatenforcer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JsonSchemaParser\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlmformatenforcer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllamacpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m     build_llamacpp_logits_processor,\n\u001b[1;32m     36\u001b[0m     build_token_enforcer_tokenizer_data,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGenerationParams\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lmformatenforcer'"
     ]
    }
   ],
   "source": [
    "from ai_calendar_processor import AICalendarProcessor\n",
    "\n",
    "processor = AICalendarProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    5135.14 ms\n",
      "llama_print_timings:      sample time =      43.79 ms /   850 runs   (    0.05 ms per token, 19409.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5944.44 ms /  1086 tokens (    5.47 ms per token,   182.69 tokens per second)\n",
      "llama_print_timings:        eval time =   25085.24 ms /   849 runs   (   29.55 ms per token,    33.84 tokens per second)\n",
      "llama_print_timings:       total time =   31756.05 ms /  1935 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To optimize the user\\'s schedule, I will first analyze the events and the user\\'s energy levels. Then, I will rearrange the events within each day to maximize productivity based on the user\\'s energy levels and the nature of each task.\\n\\nHere\\'s a Python solution to optimize the schedule:\\n\\n```python\\nimport datetime\\nimport json\\n\\n# Define the energy levels and their corresponding time ranges\\nenergy_levels = {\\n    \\'high\\': (datetime.time(9), datetime.time(12)),\\n    \\'medium\\': (datetime.time(12), datetime.time(16)),\\n    \\'low\\': (datetime.time(16), datetime.time(23))\\n}\\n\\n# Define the event types and their corresponding energy requirements\\nevent_types = {\\n    \\'high-focus\\': \\'high\\',\\n    \\'low-energy\\': \\'low\\',\\n    \\'meeting\\': \\'medium\\'\\n}\\n\\n# Function to determine the energy level of an event\\ndef get_event_energy(event):\\n    if event[\\'title\\'] == \\'Morning standup\\' or event[\\'title\\'] == \\'Code review\\':\\n        return event_types[\\'high-focus\\']\\n    elif event[\\'title\\'] == \\'Lunch break\\' or event[\\'title\\'] == \\'Email and admin\\':\\n        return event_types[\\'low-energy\\']\\n    elif event[\\'title\\'] == \\'Project planning\\' or event[\\'title\\'] == \\'Meeting\\':\\n        return event_types[\\'meeting\\']\\n    else:\\n        return \\'unknown\\'\\n\\n# Function to optimize the schedule\\ndef optimize_schedule(events):\\n    optimized_schedule = []\\n    for event in events:\\n        # Get the event\\'s energy level\\n        event_energy = get_event_energy(event)\\n        \\n        # Find the best time slot for the event based on the user\\'s energy levels\\n        best_time_slot = None\\n        for time_slot in energy_levels[event_energy]:\\n            for existing_event in optimized_schedule:\\n                if existing_event[\\'start_time\\'] < time_slot and time_slot < existing_event[\\'end_time\\']:\\n                    break\\n            else:\\n                best_time_slot = time_slot\\n                break\\n        \\n        # If a time slot is found, update the event\\'s start time\\n        if best_time_slot:\\n            event[\\'start_time\\'] = best_time_slot.strftime(\\'%H:%M\\')\\n            optimized_schedule.append(event)\\n        else:\\n            # If no time slot is found, append the event to the schedule as is\\n            optimized_schedule.append(event)\\n    \\n    return optimized_schedule\\n\\n# Input events\\nevents = [\\n    {\\n        \"id\": \"1\",\\n        \"title\": \"Morning standup\",\\n        \"start_time\": \"09:00\",\\n        \"end_time\": \"09:30\",\\n        \"day\": \"2023-05-01\"\\n    },\\n    {\\n        \"id\": \"2\",\\n        \"title\": \"Code review\",\\n        \"start_time\": \"10:00\",\\n        \"end_time\": \"11:30\",\\n        \"day\": \"2023-05-01\"\\n    },\\n    {\\n        \"id\": \"3\",\\n        \"title\": \"Lunch break\",\\n        \"start_time\": \"12:00\",\\n        \"end_time\": \"13:00\",\\n        \"day\": \"2023-05-01\"\\n    },\\n    {\\n        \"id\": \"4\",\\n        \"title\": \"Project planning\",\\n        \"start_time\": \"23:00\",\\n        \"end_time\": \"23:30\",\\n        \"day\": \"2023-05-01\"\\n    },\\n    {\\n        \"id\": \"5\",\\n        \"title\": \"Email and admin\",\\n        \"start_time\": \"16:00\",\\n        \"end_time\": \"17:00\",\\n        \"day\": \"2023-05-01\"\\n    }\\n]\\n\\n# Optimize the schedule\\noptimized_schedule = optimize_schedule(events)\\n\\n# Print the optimized schedule\\nprint(json.dumps(optimized_schedule, indent=4))\\n```\\n\\nThis code will output the optimized schedule in the specified JSON format. The schedule is optimized based on the user\\'s energy levels and the nature of each task. The events are rearranged within each day to maximize productivity.\\n\\nNote that this is a simplified solution and may not cover all possible scenarios. You may need to modify the code to fit your specific requirements.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.predict(\"\"\"[\n",
    "        {\n",
    "            \"id\": \"1\",\n",
    "            \"title\": \"Morning standup\",\n",
    "            \"start_time\": \"09:00\",\n",
    "            \"end_time\": \"09:30\",\n",
    "            \"day\": \"2023-05-01\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"2\",\n",
    "            \"title\": \"Code review\",\n",
    "            \"start_time\": \"10:00\",\n",
    "            \"end_time\": \"11:30\",\n",
    "            \"day\": \"2023-05-01\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"3\",\n",
    "            \"title\": \"Lunch break\",\n",
    "            \"start_time\": \"12:00\",\n",
    "            \"end_time\": \"13:00\",\n",
    "            \"day\": \"2023-05-01\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"4\",\n",
    "            \"title\": \"Project planning\",\n",
    "            \"start_time\": \"23:00\",\n",
    "            \"end_time\": \"23:30\",\n",
    "            \"day\": \"2023-05-01\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"5\",\n",
    "            \"title\": \"Email and admin\",\n",
    "            \"start_time\": \"16:00\",\n",
    "            \"end_time\": \"17:00\",\n",
    "            \"day\": \"2023-05-01\"\n",
    "        }\n",
    "    ]\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/druhi/mambaforge/envs/vidya/lib/python3.11/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/druhi/mambaforge/envs/vidya/lib/python3.11/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_path\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/druhi/mambaforge/envs/vidya/lib/python3.11/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/druhi/mambaforge/envs/vidya/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from /Users/druhi/Library/Caches/llama_index/models/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  8137.66 MiB, ( 8137.73 / 36864.00)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   532.31 MiB\n",
      "llm_load_tensors:      Metal buffer size =  8137.65 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 64000\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  8000.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 8000.00 MiB, K (f16): 4000.00 MiB, V (f16): 4000.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =  4157.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   133.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'quantize.imatrix.chunks_count': '125', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '4096', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'general.finetune': 'Instruct', 'general.file_type': '7', 'llama.block_count': '32', 'general.size_label': '8B', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '14336', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'llama3.1', 'llama.attention.head_count': '32', 'quantize.imatrix.entries_count': '224', 'llama.context_length': '131072', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.name': 'Meta Llama 3.1 8B Instruct'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing index...\n"
     ]
    }
   ],
   "source": [
    "from ai_enlightened_chatbot import RAGAgent\n",
    "agentio = RAGAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 65 prefix-match hit, remaining 5652 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <<SYS>>\n",
      " Man's duty to man is a complex and multifaceted question that has been debated by philosophers throughout history. In the context of various philosophical traditions, theories, and methodologies, different perspectives on this question have emerged.\n",
      "\n",
      "One perspective, rooted in the Stoic tradition, emphasizes the importance of living in accordance with reason and virtue. According to this view, man's duty to man is to cultivate inner strength, wisdom, and self-control, and to act in ways that promote the common good. This perspective is exemplified in the teachings of Epictetus, who argued that individuals should focus on things within their control and strive to live in harmony with nature.\n",
      "\n",
      "Another perspective, influenced by the ideas of Immanuel Kant, emphasizes the importance of treating others as ends in themselves, rather than mere means to an end. According to this view, man's duty to man is to respect the inherent dignity and worth of every human being, and to act in ways that promote their autonomy and freedom. This perspective is reflected in Kant's categorical imperative, which posits that individuals should act only in accordance with maxims that could be willed as universal laws.\n",
      "\n",
      "In contrast, the utilitarian tradition, as represented by John Stuart Mill, emphasizes the importance of promoting the greatest happiness for the greatest number. According to this view, man's duty to man is to act in ways that maximize overall well-being and minimize suffering. This perspective is reflected in Mill's concept of the \"happiness principle,\" which posits that individuals should act in ways that promote the greatest happiness for all.\n",
      "\n",
      "The existentialist tradition, as represented by Jean-Paul Sartre, emphasizes the importance of individual freedom and choice. According to this view, man's duty to man is to take responsibility for one's own actions and to create one's own values in life. This perspective is reflected in Sartre's concept of \"existence precedes essence,\" which posits that individuals must create their own meaning and purpose in life.\n",
      "\n",
      "Finally, the virtue ethics tradition, as represented by Aristotle, emphasizes the importance of cultivating virtues such as compassion, justice, and wisdom. According to this view, man's duty to man is to develop these virtues and to act in ways that promote the common good. This perspective is reflected in Aristotle's concept of \"eudaimonia,\" which posits that individuals should strive to live a life of happiness and fulfillment.\n",
      "\n",
      "In conclusion, the question of man's duty to man is a complex and multifaceted issue that has been debated by philosophers throughout history. Different perspectives on this question have emerged, each with its own unique insights and challenges. Ultimately, the answer to this question depends on one's philosophical commitments and values, and requires careful consideration of the various perspectives and arguments that have been put forward.\n",
      "\n",
      "In the words of the ancient Greek philosopher, Aristotle, \"What is man's duty to man?\" is a question that requires us to \"examine the virtues and the vices, and to consider what is the mean between them.\" By engaging in this kind of critical analysis and reflection, we can gain a deeper understanding of our duties to one another and strive to live a life of virtue and compassion.\n",
      "\n",
      "As the great philosopher, Immanuel Kant, once said, \"The question is not, what is my duty to man, but what is my duty to myself, and what is my duty to the moral law.\" By focusing on our own moral agency and the universal principles of morality, we can cultivate a sense of duty to others and strive to live a life of integrity and virtue.\n",
      "\n",
      "In the end, the question of man's duty to man is a question that requires us to engage in ongoing reflection and inquiry. It is a question that challenges us to think critically about our values and commitments, and to strive for a deeper understanding of our duties to one another. By embracing this challenge, we can cultivate a sense of compassion, empathy, and responsibility, and strive to live a life of virtue and fulfillment."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1180.25 ms\n",
      "llama_print_timings:      sample time =      54.78 ms /   808 runs   (    0.07 ms per token, 14748.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9244.12 ms /  5652 tokens (    1.64 ms per token,   611.42 tokens per second)\n",
      "llama_print_timings:        eval time =   34377.62 ms /   807 runs   (   42.60 ms per token,    23.47 tokens per second)\n",
      "llama_print_timings:       total time =   45130.34 ms /  6459 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response: None\n",
      "\n",
      "============================================================\n",
      "\n",
      "References:\n",
      "Source text: of human actions. This is the domain of ca suistry. Secondly, the aspects under which the \n",
      "m o s t  g e n e r a l  p r i n c i p l e s  o f  m o r a l s  m a y  b e  p r e s e n t e d  t o  u s  a r e...\n",
      "Page number: 2550\n",
      "Filename: plato-complete-works.pdf\n",
      "--------------------------------------------\n",
      "Source text: about, and how to question and what is the way to arrange the \n",
      "questions; moreover, as to the question what use is served by all \n",
      "arguments of this kind, and concerning the answerer’s part, \n",
      "both a...\n",
      "Page number: 599\n",
      "Filename: aristotle-the-complete.pdf\n",
      "--------------------------------------------\n",
      "Source text: The question whether the ruler or statesman should be a philosopher is one that has not \n",
      "lost interest in modern times. In most count ries of Europe and Asia there has been some \n",
      "one in the course of ...\n",
      "Page number: 1432\n",
      "Filename: plato-complete-works.pdf\n",
      "--------------------------------------------\n",
      "Source text: maker. What Bacon seems to promise him he  will find realized in the great German \n",
      "thinker, an emancipation nearly complete from  the influences of the scholastic logic. \n",
      "3. Many of those who are leas...\n",
      "Page number: 2305\n",
      "Filename: plato-complete-works.pdf\n",
      "--------------------------------------------\n",
      "Source text: In the Hegelian system ideas supersede persons. The world of thought, though \n",
      "sometimes described as Spirit or ‘Geist,’ is  really impersonal. The minds of men are to \n",
      "be regarded as one mind, or more...\n",
      "Page number: 2302\n",
      "Filename: plato-complete-works.pdf\n",
      "--------------------------------------------\n",
      "Source text: The questions that the \n",
      "Meditations\n",
      " tries to answer are\n",
      "primarily metaphysical and ethical ones: Why are we here?\n",
      "How should we live our lives? How can we ensure that we\n",
      "do what is right? How can we ...\n",
      "Page number: 32\n",
      "Filename: marcus-aurelius-meditations.pdf\n",
      "--------------------------------------------\n",
      "Source text: position against which one is arguing be quite definite, and the \n",
      "claim that he shall say only what he thinks, create abundant \n",
      "opportunity for drawing him into paradox or fallacy, and also, \n",
      "wheth...\n",
      "Page number: 561\n",
      "Filename: aristotle-the-complete.pdf\n",
      "--------------------------------------------\n",
      "Source text: Aristotle – Topics \n",
      " \n",
      "[Translated by W. A. Pickard-Cambridge] \n",
      " \n",
      " \n",
      " \n",
      "Book I \n",
      " \n",
      "1 Our treatise proposes to find a line of inquiry whereby we shall \n",
      "be able to reason from opinions that are generally...\n",
      "Page number: 326\n",
      "Filename: aristotle-the-complete.pdf\n",
      "--------------------------------------------\n",
      "Source text: important questions, distrusting ourselves as not being equal to \n",
      "deciding. \n",
      "We deliberate not about ends but about means. For a doctor \n",
      "does not deliberate whether he shall heal, nor an orator \n",
      "w...\n",
      "Page number: 2587\n",
      "Filename: aristotle-the-complete.pdf\n",
      "--------------------------------------------\n",
      "Source text: state of the world, not wholly evil or wholly  good, is supposed to be  a witness. More we \n",
      "might desire to have, but are not permi tted. Though a human tyrant would be \n",
      "intolerable, a divine tyrant i...\n",
      "Page number: 2559\n",
      "Filename: plato-complete-works.pdf\n",
      "--------------------------------------------\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agentio.query(\"What is man's duty to man?\", \"philosopher\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
